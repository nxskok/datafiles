# (C33) when do the pooled and Welch $t$-tests give different results?

(a) We are going to be generating random data in this question. To ensure that you get the same random data every time you run the code, at least if you run all your code from the beginning, run this: 

```{r}
id <- 863900015
```


```{r}
set.seed(id)
```

where in place of `id` you put your student number.


(a) The function `rnorm` generates random data from a normal distribution. It has three inputs: the sample size, the population mean, and the population. Use `rnorm` to generate a random sample of size 10 from a normal distribution with mean 15 and SD 2. Store this in a vector `z1` and display the result.

Solution

This code, therefore:

```{r}
z1 <- rnorm(10, 15, 2)
z1
```

The values you get should all or almost all be between 10 and 20, but they will be different from mine, because your student number is different from mine. Your friends will also have different student numbers from you, so their random sample will be different from yours.^[It is therefore a suspicious sign if their sample is the *same* as yours.]

(a) Generate a second random normal sample, this time of size 20, with mean 12 and SD 1. Call this one `z2`.

Solution

Copy, paste, and edit:

```{r}
z2 <- rnorm(20, 14, 1)
z2
```

This time, the values seem to be close to 12, as they should be.

(a) The two-sample $t$-tests can be run, as we have seen, on a dataframe that has a column of data values and a column indicating which group each value belongs to. They can also be run with the two columns as input. Run a Welch two-sample $t$-test on your `z1` and `z2`, with a two-sided alternative hypothesis. What do you conclude?

Solution

Two-sided is the default, so just pass in the two vectors:

```{r}
t.test(z1, z2)
```

I (correctly) conclude that the two samples have different means, though only just, since the P-value is only just less than 0.05. Your conclusion might be different, depending on the random numbers your seed produces. (In this case, we know what the population means are; they are different, so we are hoping to reject.)


(a) Rerun your test of the previous part, but using a pooled variance. How do the P-values compare? Which test is better? Explain briefly.

Solution

Add the `var.equal` thing:

```{r}
t.test(z1, z2, var.equal = TRUE)
```

This time the P-value is much smaller, and gives much stronger evidence that the means are different. But this is dishonest; we know that the population variances are different, so that the Welch test is the right one to use, not the pooled test.

Your results may be different; describe what you have. You may even find that the pooled test correctly rejects while the Welch one does not, but the Welch test is still the appropriate one; the smaller P-value of the pooled test is dishonestly small.

Extra: we probably haven't seen the ideas below in class yet, but I wanted to get a sense of what the results might look like for you, with different random numbers. The idea is to repeat the drawing of `z1` and `z2` a lot of times, and run both $t$-tests every time:

```{r}
tibble(sim = 1:1000) %>% 
  rowwise() %>% 
  mutate(z1 = list(rnorm(10, 15, 2))) %>% 
  mutate(z2 = list(rnorm(20, 14, 1))) %>% 
  mutate(welch = list(t.test(z1, z2))) %>% 
  mutate(pooled = list(t.test(z1, z2, var.equal = TRUE))) %>% 
  mutate(p_welch = welch$p.value, p_pooled = pooled$p.value) %>% 
  ggplot(aes(x = p_welch, y = p_pooled)) + geom_point() +
  geom_abline(intercept = 0, slope = 1)
```

The line is $y = x$, so a point on the line represents samples that gave the same P-values for the Welch and pooled tests. Almost always, though, the points are *below* the line, indicating that the (wrong) pooled test almost always gives a smaller P-value for the same data, one that might end up with you rejecting a null of equal population means when the data really don't support it. (By that I mean that the sample sizes are not really big enough to have any great chance of rejecting equal means.) 

(a) Generate some more random normal data, this time a sample of size 30 with mean 16 and standard deviation 5. Save this in `z3`.

Solution 

Same idea as the others:

```{r}
z3 <- rnorm(30, 16, 5)
z3
```

These should look more variable than the others.

(a) Run the Welch and pooled $t$-tests for comparing `z1` and `z3`, and compare the P-values.

Solution

The same idea as before:

```{r}
t.test(z1, z3, var.equal = TRUE)
t.test(z1, z3)
```

In this case, the P-values for me, 0.8842 and 0.813, are both a long way from significance and both point to the same conclusion. The Welch test is again the appropriate one, since the population variances are again different, but this time the pooled test P-value is bigger.

Extra: I wanted to get a sense of what you might see, so I did some simulations again (the code is copy-paste-edited):

```{r}
tibble(sim = 1:1000) %>% 
  rowwise() %>% 
  mutate(z1 = list(rnorm(10, 15, 2))) %>% 
  mutate(z3 = list(rnorm(30, 16, 5))) %>% 
  mutate(welch = list(t.test(z1, z3))) %>% 
  mutate(pooled = list(t.test(z1, z3, var.equal = TRUE))) %>% 
  mutate(p_welch = welch$p.value, p_pooled = pooled$p.value) %>% 
  ggplot(aes(x = p_welch, y = p_pooled)) + geom_point() +
  geom_abline(intercept = 0, slope = 1)
```

This time the pooled P-value is almost always *bigger* than the Welch one, when run on the same data. Once again, the Welch test is the right one, but the wrong P-value is now too big. 

(a) Based on the results of your tests, what do you think is the most dangerous situation to (wrongly) use the pooled $t$-test in place of the Welch one? Explain briefly.

Solution

I'm hoping you found similar results to mine: in the first case, the pooled test gave a P-value that was *too small*, and in the second one it gave a P-value that was *too big*. (If not, comment on what you saw.)

As a general principle, giving a P-value that is smaller than it should be is dangerous, because it might make you think that you have a significant result when you actually don't. (Some people call this a "false discovery".) Or, even if it doesn't change the decision about significance, it can make you think that the evidence is stronger than it actually is. This is the situation we had when comparing `z1` and `z2`.

Giving a P-value that is *larger* than it should be is also undesirable, but it is not dangerous in the same way that the previous case was; the effect might be to make you miss out on a significant result, but this is not as bad as making a decision based on a "significant" result that actually wasn't. Having a P-value that is higher than it should be is what statisticians call "conservative"; it conserves the status quo more often than it should. This is the situation we had when comparing `z1` and `z3`.

How were these two comparison different? Let's go back and compare the code that generated the `z` values:

```{r eval=FALSE}
z1 <- rnorm(10, 15, 2)
z2 <- rnorm(20, 14, 1)
z3 <- rnorm(30, 16, 5)
```

The means only determine whether the comparison typically comes out significant or not. (The results would be the same whatever the means where.) The things that matter are the sample size and the SD (or variance).

In the first comparison, between `z1` and `z2`, the group (`z2`) that had the larger sample size had the smaller SD. But in the second one, between `z1` and `z3`, the group (`z3`) that had the larger sample size also had the larger SD. So the dangerous case for the pooled test, where it can give you a P-value that is lower than it should be and make you think that your evidence is actually stronger than it really is, is the first one: where the larger sample size goes with the smaller SD (or the smaller sample size goes with the larger SD).

This, I hope, makes some kind of sense: if the smaller group is more variable, it ought to be difficult to amass evidence that the smaller group differs from the larger one --- you really don't know much about the mean of the smaller group in this case. (Think about what a CI for its mean would look like: it would be long both because the sample values are going to be variable and because the sample size is small.) The pooled test, however, because it works on the assumption that both groups have the same variance,  will make you think that you know more about the small group than you actually do. The second case, where the smaller group also has the smaller SD, equalizes things out a bit: you know something about both group means --- the big group because it has a large sample size, the small group because its values are not all that spread out. 

think about doing this with actual data like p 519 of utts (simulated), eg arts and science students instead of males and females
